{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AG Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is structured as follows:\n",
    "<br>__Data Analysis & Review__ - This section explains the data being used\n",
    "<br>__Modelling__ - This section contains the predictive modelling\n",
    "<br>__Results & Conclusions__ Wrap up of all information discovered and learnt in this analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import os\n",
    "from enum import Enum\n",
    "import pickle\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt \n",
    "import ppscore as pps\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def cell_last_run():\n",
    "    \"\"\"\n",
    "    Databricks notebooks tell you the last time you ran a cell\n",
    "    which I find very useful to know, it's very easy with\n",
    "    notebooks to forget to re-run cells after changes,\n",
    "    now the cell running order can be checked\n",
    "    \"\"\"  \n",
    "    print(f\"cell run at {datetime.datetime.now().strftime('%H:%M:%S %Y_%m_%d')} \\n\")\n",
    "\n",
    "cell_last_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Snowflake and Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "class FileEnum(Enum):\n",
    "    data_parent_folder_name = 'data'\n",
    "    raw_data_folder_name = 'raw'\n",
    "    original_data_folder_name = 'original'\n",
    "    processed_data_folder_name = 'processed'\n",
    "    raw_data_filename = ''\n",
    "    training_data_filename = 'train.csv'\n",
    "    testing_data_filename = 'test.csv'\n",
    "    testing_folder_name = 'test'\n",
    "    training_folder_name = 'train'\n",
    "    artifacts_folder_name = 'artifacts'\n",
    "    labels_filename = 'labels'\n",
    "    \n",
    "def get_artifact_folder() -> Path:\n",
    "    return Path.joinpath(Path(os.getcwd()).parent, FileEnum.artifacts_folder_name.value)\n",
    "    \n",
    "    \n",
    "def get_data_type_folder(data_type:str):\n",
    "    if not data_type in ['raw', 'processed']:\n",
    "        raise ValueError('data type not recognised')\n",
    "        \n",
    "    if data_type == 'raw':\n",
    "        return FileEnum.raw_data_folder_name.value\n",
    "    elif data_type == 'processed':\n",
    "        return FileEnum.processed_data_folder_name.value\n",
    "    \n",
    "def save_test_data(test_data: pd.DataFrame, data_type: str):\n",
    "\n",
    "    test_data_path = Path.joinpath(get_data_folder_path(),\n",
    "                                    get_data_type_folder(data_type=data_type),\n",
    "                                    FileEnum.testing_folder_name.value,\n",
    "                                    FileEnum.testing_data_filename.value)\n",
    "\n",
    "    if Path.is_file(test_data_path):\n",
    "        print('data already saved, no action performed')\n",
    "    else:\n",
    "        test_data.to_csv(test_data_path)\n",
    "        \n",
    "def save_train_data(train_data: pd.DataFrame, data_type: str):\n",
    "      \n",
    "    test_data_path = Path.joinpath(get_data_folder_path(),\n",
    "                                        get_data_type_folder(data_type=data_type),\n",
    "                                        FileEnum.training_folder_name.value,\n",
    "                                        FileEnum.training_data_filename.value)\n",
    "    \n",
    "    if Path.is_file(test_data_path):\n",
    "        print('data already saved, no action performed')\n",
    "    else:\n",
    "        test_data.to_csv(test_data_path)\n",
    "    \n",
    "    \n",
    "def get_data_folder_path() -> Path:\n",
    "    \"\"\"\n",
    "    folder data assumed to be in cirrent directory, if not please\n",
    "    update this function\n",
    "    \"\"\"\n",
    "    return Path.joinpath(Path(os.getcwd()).parent, FileEnum.data_parent_folder_name.value)\n",
    "\n",
    "def get_original_data_file() -> Path:\n",
    "    \"\"\"\n",
    "    folder data assumed to be in current directory, if not please\n",
    "    update this function\n",
    "    \"\"\"\n",
    "    return Path.joinpath(get_data_folder_path(),\n",
    "                                 FileEnum.raw_data_folder_name.value,\n",
    "                                 FileEnum.original_data_folder_name.value,\n",
    "                                 FileEnum.raw_data_filename.value)\n",
    "    \n",
    "\n",
    "def get_train_data_file(data_type: str) -> Path:\n",
    "    \"\"\"\n",
    "    folder data assumed to be in current directory, if not please\n",
    "    update this function\n",
    "    \"\"\"\n",
    "        \n",
    "    return Path.joinpath(get_data_folder_path(),\n",
    "                             get_data_type_folder(data_type=data_type),\n",
    "                             FileEnum.training_folder_name.value,\n",
    "                             FileEnum.training_data_filename.value)\n",
    "        \n",
    "\n",
    "def get_test_data_file(data_type: str) -> Path:\n",
    "    \"\"\"\n",
    "    folder data assumed to be in current directory, if not please\n",
    "    update this function\n",
    "    \"\"\"\n",
    "    \n",
    "    return Path.joinpath(get_data_folder_path(),\n",
    "                             get_data_type_folder(data_type=data_type),\n",
    "                             FileEnum.testing_folder_name.value,\n",
    "                             FileEnum.testing_data_filename.value)\n",
    "\n",
    "\n",
    "def import_data(data_type: str, train_or_test: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function handles importing of data for training the model\n",
    "    :return pd.DataFrame: training data for model or curating data\n",
    "    \"\"\"\n",
    "    \n",
    "    if not data_type in ['raw', 'processed']:\n",
    "        raise ValueError('data type not recognised')\n",
    "        \n",
    "    if data_type == 'processed'and not train_or_test in ['train', 'test']:\n",
    "        raise ValueError('data must be train or test')\n",
    "        \n",
    "    if train_or_test == 'train':\n",
    "        return pd.read_csv(get_train_data_file(data_type=data_type), index_col=['uid'])\n",
    "    elif train_or_test == 'test':\n",
    "        return pd.read_csv(get_test_data_file(data_type=data_type), index_col=['uid'])\n",
    "    elif not train_or_test:\n",
    "        return pd.read_csv(get_original_data_file())\n",
    "    \n",
    "\n",
    "def create_train_test_sets(random_seed: int = 3142, training_set_proportion: float = 0.9):\n",
    "    \"\"\"\n",
    "    splits the data into train and test\n",
    "    \"\"\"\n",
    "    \n",
    "    DATA_TYPE = 'raw'\n",
    "    \n",
    "    if Path.is_file(get_train_data_file(data_type=DATA_TYPE)):\n",
    "        print('data already split, no action performed')\n",
    "        return None\n",
    "    \n",
    "    # get the data\n",
    "    all_data: pd.DataFrame = import_data(data_type='raw')\n",
    "    \n",
    "    # get training data\n",
    "    train_data = all_data.sample(frac=training_set_proportion, random_state = random_seed)\n",
    "    \n",
    "    # get testing data\n",
    "    \n",
    "    train_uids = set(train_data['uid'])\n",
    "    test_data_mask = all_data.apply(lambda row: False if row.uid in train_uids else True, axis=1)\n",
    "    test_data = all_data[test_data_mask]\n",
    "    \n",
    "    # ensure we have not dropped any data\n",
    "    assert test_data.shape[0] + train_data.shape[0] == all_data.shape[0]\n",
    "    \n",
    "    #set the uid as the index\n",
    "    test_data.set_index(keys='uid', drop=True, inplace=True)\n",
    "    train_data.set_index(keys='uid', drop=True, inplace=True)\n",
    "    \n",
    "    train_data.to_csv(get_train_data_file(data_type=DATA_TYPE))\n",
    "    test_data.to_csv(get_test_data_file(data_type=DATA_TYPE))\n",
    "    \n",
    "    print('split complete.')\n",
    "    \n",
    "def save_artifacts(artifact: Any, filename: str):\n",
    "    \n",
    "    with open(Path.joinpath(get_artifact_folder(), filename), 'wb') as f:\n",
    "        pickle.dump(artifact, f)\n",
    "        \n",
    "def get_artifacts(filename: str) -> Any:\n",
    "    \n",
    "    with open(Path.joinpath(get_artifact_folder(), filename), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "def pretty_plot(data: pd.DataFrame, x: str=None, y=None, title=None):\n",
    "    \"\"\"\n",
    "    set up printing of data \n",
    "    \"\"\"\n",
    "    if x:\n",
    "        data.plot(x=x, y=y, figsize=(20, 5))\n",
    "    else:\n",
    "        data.plot(y=y, figsize=(20, 5), use_index=True)\n",
    "    if title:\n",
    "        ax = plt.gca()\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(0.8, 0.2, 0.2, -0.5))\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not want to perform EDA on data in the test set, however some idea of the data is needed in order to correctly identify a testing set. This is what the initial data review is. Here I will get some idea of what the data looks like to decide how to split the data into train and test then traditional EDA techniques will be applied only the training data.\n",
    "\n",
    "This approach reduces as much as possible any potential bias introduced into the preprocessing of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "all_data: pd.DataFrame = import_data(data_type='raw')\n",
    "\n",
    "\n",
    "print(f\"number of rows = {all_data.shape[0]}\")\n",
    "print(f\"number of columns = {all_data.shape[1]}\")\n",
    "\n",
    "all_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a quick review of the data we can see we have mixture of categorical variables, boolean variables, and some ordinal data. There is some missing data in some of the columns but that does not look to be a large issue. There is no dates attached to the data so in this analysis we are not able to take the temporal aspect of the data into account. It however may be useful to know the date/time of this data in future since distributional shift in both the predictors and labels can occur over time (and I suspect patterns of behaviour and spending during lockdown may not be representative of 'normal' times).\n",
    "\n",
    "We have a uid column which may be useful to connect data in the future, obviously we do not want to put this into the model but it will be useful to keep around so this can be used as the index to the dataframe.\n",
    "\n",
    "We have of the order of 10's of thousands of rows so there should be enough data for modelling possibly this might not be enough for a deep learning appraoch so will keep to more traditional modelling appraoches. Given the amount of data we should be find with removing 10% of the data for the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "create_train_test_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the test data set has been removed full EDA can be performed without the risk of biasing and preprocessing decisions. Next the training data will be imported and profiled using pandas profiling to investigate the data to see if any additional proprocessing is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "train_data = import_data(data_type='raw', train_or_test='train')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "profile = ProfileReport(train_data, title='Pandas Profiling Report')\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the overview of the data first there is a small amount of missing cells and no duplicate rows so this data set looks quite rich. Although an imputer may be needed to deal with missing data - this can be investigated when looking at the individual features.\n",
    "\n",
    "\n",
    "Investigating the warnings we see:\n",
    "\n",
    "\n",
    "All the other features have warnings aroun the number of zeros - since these represent counts of real world numbers this is likely not a problem although will investigate individually below.\n",
    "\n",
    "Feature analysis (looking at the 'variable' tab of the profile widget):\n",
    "\n",
    "\n",
    "Correlations:\n",
    "\n",
    "Pandas profiling only produces correlations for variables whose dtypes are numeric (here already the numeric encoding of week days has already caused an issue). Correlation is less well defined for binary variables (once categorical variables are one hot encoded each feautre essentially becomes a binary feature). More work will be needed to understand the relationships between the categorical data.\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interim Discussion of EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove unwated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "cols_to_keep: list = ['']\n",
    "\n",
    "feature_data = train_data[cols_to_keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "imp = SimpleImputer(missing_values=float('nan'), strategy='constant', fill_value='missing')\n",
    "\n",
    "feature_data_eda = pd.DataFrame(imp.fit_transform(feature_data), columns=feature_data.columns, index=feature_data.index)\n",
    "\n",
    "save_artifacts(imp, 'imputer.pkl')\n",
    "\n",
    "feature_data_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class WeekdayEncoder(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.mapping = {0: 'sunday',\n",
    "                       1: 'monday',\n",
    "                       2: 'tuesday',\n",
    "                       3: 'wednesday',\n",
    "                       4: 'thursday',\n",
    "                       5: 'friday',\n",
    "                       6: 'saturday'}\n",
    "        \n",
    "    def fit(self, *args, **kwargs):\n",
    "        pass\n",
    "        \n",
    "    def fit_transform(self, X, *args, **kwargs):\n",
    "        return self.transform(X)\n",
    "        \n",
    "    def transform(self, X:pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        weekday = X['weekday'].apply(lambda x: self.mapping.get(x))\n",
    "        \n",
    "        X.drop(columns = ['weekday'], inplace=True)\n",
    "        weekday = weekday.rename('weekday')\n",
    "        \n",
    "        return pd.merge(X, weekday, left_index=True, right_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "wkd = WeekdayEncoder()\n",
    "\n",
    "save_artifacts(wkd, 'weekday_transformer.pkl')\n",
    "\n",
    "feature_data_eda = wkd.transform(feature_data_eda)\n",
    "feature_data_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Relationships Between Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pandas profiling only produces correlations for columns it perceives to be numerical currently the realtionships between the remaining features are unknown. Traditional correlation is not a very usefull tool to resolve this is the predictive power score -more information can be found here https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598?gi=b742e6b11335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_last_run()\n",
    "\n",
    "pps_matrix = pps.matrix(feature_data_eda)\n",
    "\n",
    "#remove uninteresting scores\n",
    "pps_matrix = pps_matrix[pps_matrix['ppscore'].apply(lambda x: x > 0.0001)]\n",
    "\n",
    "pps_matrix.sort_values('ppscore', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preictive power scores above (ignoring the perfect scores of variables against themselves) none of the scores appear high enough for any concern. The closest realtionship is country and language learnt which I suspect is due to most non-English speaking countries electing to learn English so some realtionship there is anecdotally expected. Similarly with country and motivation have similar ppscores in both directions. Also attribtuion and country have a relationship in both directions - this could be due to explicit marketing straegies (or lack thereof in certain countries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data Ready for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "busuu-env",
   "language": "python",
   "name": "busuu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
